{
  "hash": "dbc68c184486dad2a3d25c51c8272793",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Final Code\"\nformat: html\noutput: html_document\n---\n\n\n\n#### Load Dataset and check for empty values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load dataset\ndata <- read.csv('C:/Users/adikh/OneDrive/Desktop/Stat/AnkitBorle/Dataset.csv')\nsum(is.na(data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n#### Check for duplicate rows\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for duplicate rows\nduplicate_rows <- sum(duplicated(data))\nduplicate_rows\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n#### Total number of rows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(complete.cases(data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1000\n```\n\n\n:::\n:::\n\n\n#### Columns in Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n# Rename the columns\ndata <- data %>%\n  rename(\n    Patient_Number = No_Pation,\n    Creatinine = Cr,\n    HbA1c_Level = HbA1c,\n    Cholesterol = Chol,\n    Triglycerides = TG,\n    HDL_Cholesterol = HDL,\n    LDL_Cholesterol = LDL,\n    VLDL_Cholesterol = VLDL\n  )\n\ncolnames(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"ID\"               \"Patient_Number\"   \"Gender\"           \"AGE\"             \n [5] \"Urea\"             \"Creatinine\"       \"HbA1c_Level\"      \"Cholesterol\"     \n [9] \"Triglycerides\"    \"HDL_Cholesterol\"  \"LDL_Cholesterol\"  \"VLDL_Cholesterol\"\n[13] \"BMI\"              \"CLASS\"           \n```\n\n\n:::\n:::\n\n\n#### Removing leading and trailing spaces\n\nBefore removing the spaces\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(data$CLASS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  N  N    P   Y  Y  \n102   1  53 840   4 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply trimws to all columns in the dataset\ndata <- data.frame(lapply(data, function(x) {\n  if (is.character(x)) {\n    trimws(x) # Trim whitespace for character columns\n  } else {\n    x # Leave other columns unchanged\n  }\n}), stringsAsFactors = FALSE)\n```\n:::\n\n\nAfter removing the spaces\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(data$CLASS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  N   P   Y \n103  53 844 \n```\n\n\n:::\n:::\n\n\n#### Calculate summary statistics (mean, median, standard deviation) for BMI and lipid profiles (LDL, HDL, TG), stratified by CLASS (diabetic vs. non-diabetic).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Assuming `data` is your dataframe loaded into R\n# Group by CLASS and calculate summary statistics\nsummary_stats <- data %>%\n  group_by(CLASS) %>%\n  summarise(\n    BMI_mean = mean(BMI, na.rm = TRUE),\n    BMI_median = median(BMI, na.rm = TRUE),\n    BMI_sd = sd(BMI, na.rm = TRUE),\n    LDL_mean = mean(LDL_Cholesterol, na.rm = TRUE),\n    LDL_median = median(LDL_Cholesterol, na.rm = TRUE),\n    LDL_sd = sd(LDL_Cholesterol, na.rm = TRUE),\n    HDL_mean = mean(HDL_Cholesterol, na.rm = TRUE),\n    HDL_median = median(HDL_Cholesterol, na.rm = TRUE),\n    HDL_sd = sd(HDL_Cholesterol, na.rm = TRUE),\n    TG_mean = mean(Triglycerides, na.rm = TRUE),\n    TG_median = median(Triglycerides, na.rm = TRUE),\n    TG_sd = sd(Triglycerides, na.rm = TRUE)\n  )\n\n# View the results\nsummary_stats %>%\n  kable(format = \"html\", digits = 2) %>%\n  kable_styling(font_size = 10)  # Adjust the font size as needed\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 10px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> CLASS </th>\n   <th style=\"text-align:right;\"> BMI_mean </th>\n   <th style=\"text-align:right;\"> BMI_median </th>\n   <th style=\"text-align:right;\"> BMI_sd </th>\n   <th style=\"text-align:right;\"> LDL_mean </th>\n   <th style=\"text-align:right;\"> LDL_median </th>\n   <th style=\"text-align:right;\"> LDL_sd </th>\n   <th style=\"text-align:right;\"> HDL_mean </th>\n   <th style=\"text-align:right;\"> HDL_median </th>\n   <th style=\"text-align:right;\"> HDL_sd </th>\n   <th style=\"text-align:right;\"> TG_mean </th>\n   <th style=\"text-align:right;\"> TG_median </th>\n   <th style=\"text-align:right;\"> TG_sd </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> N </td>\n   <td style=\"text-align:right;\"> 22.37 </td>\n   <td style=\"text-align:right;\"> 22 </td>\n   <td style=\"text-align:right;\"> 1.42 </td>\n   <td style=\"text-align:right;\"> 2.63 </td>\n   <td style=\"text-align:right;\"> 2.6 </td>\n   <td style=\"text-align:right;\"> 0.98 </td>\n   <td style=\"text-align:right;\"> 1.23 </td>\n   <td style=\"text-align:right;\"> 1.1 </td>\n   <td style=\"text-align:right;\"> 0.51 </td>\n   <td style=\"text-align:right;\"> 1.63 </td>\n   <td style=\"text-align:right;\"> 1.3 </td>\n   <td style=\"text-align:right;\"> 1.03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:right;\"> 23.93 </td>\n   <td style=\"text-align:right;\"> 24 </td>\n   <td style=\"text-align:right;\"> 2.71 </td>\n   <td style=\"text-align:right;\"> 2.49 </td>\n   <td style=\"text-align:right;\"> 2.5 </td>\n   <td style=\"text-align:right;\"> 0.87 </td>\n   <td style=\"text-align:right;\"> 1.13 </td>\n   <td style=\"text-align:right;\"> 1.0 </td>\n   <td style=\"text-align:right;\"> 0.38 </td>\n   <td style=\"text-align:right;\"> 2.13 </td>\n   <td style=\"text-align:right;\"> 1.8 </td>\n   <td style=\"text-align:right;\"> 1.06 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Y </td>\n   <td style=\"text-align:right;\"> 30.81 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 4.32 </td>\n   <td style=\"text-align:right;\"> 2.62 </td>\n   <td style=\"text-align:right;\"> 2.5 </td>\n   <td style=\"text-align:right;\"> 1.14 </td>\n   <td style=\"text-align:right;\"> 1.21 </td>\n   <td style=\"text-align:right;\"> 1.1 </td>\n   <td style=\"text-align:right;\"> 0.69 </td>\n   <td style=\"text-align:right;\"> 2.45 </td>\n   <td style=\"text-align:right;\"> 2.1 </td>\n   <td style=\"text-align:right;\"> 1.43 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n#### Machine learning models for predicting the class of diabetes (N, P, Y) based on clinical and demographic factors?\n\n### Random Forest\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata$CLASS <- as.factor(data$CLASS)\n\nset.seed(42)  # For reproducibility\n# Split data -> 80% as training data and 20% as testing data\ntrain_index <- createDataPartition(data$CLASS, p = 0.8, list = FALSE)\ntrain_data <- data[train_index, ]\ntest_data <- data[-train_index, ]\n\n# Fit a Random Forest model\nrf_model <- randomForest(CLASS ~ ., data = train_data, ntree = 100, importance = TRUE,random_state = 42)\n\n# Make predictions on the test set\npredictions <- predict(rf_model, test_data)\n\n# Evaluate the model\nconf_matrix <- confusionMatrix(predictions, test_data$CLASS)\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   N   P   Y\n         N  20   0   2\n         P   0  10   1\n         Y   0   0 165\n\nOverall Statistics\n                                          \n               Accuracy : 0.9848          \n                 95% CI : (0.9564, 0.9969)\n    No Information Rate : 0.8485          \n    P-Value [Acc > NIR] : 5.887e-11       \n                                          \n                  Kappa : 0.9457          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: N Class: P Class: Y\nSensitivity            1.0000  1.00000   0.9821\nSpecificity            0.9888  0.99468   1.0000\nPos Pred Value         0.9091  0.90909   1.0000\nNeg Pred Value         1.0000  1.00000   0.9091\nPrevalence             0.1010  0.05051   0.8485\nDetection Rate         0.1010  0.05051   0.8333\nDetection Prevalence   0.1111  0.05556   0.8333\nBalanced Accuracy      0.9944  0.99734   0.9911\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get variable importance from the fitted model\nvar_importance <- randomForest::importance(rf_model)\n\n# Print variable importance\nprint(var_importance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                           N          P          Y MeanDecreaseAccuracy\nID                2.90033107  3.0463945  2.2066249            4.0638413\nPatient_Number    3.60284624  3.1698481  6.1876847            6.9406985\nGender            0.62418016  1.4389186  0.9721012            1.7955657\nAGE               5.27502908  8.7274922  7.7950173           10.1960232\nUrea              0.05947339  0.4007783  3.4196877            2.2489929\nCreatinine       -0.12163660  2.6791981  3.4951424            3.4406207\nHbA1c_Level      25.83558650 12.7027524 11.1660087           23.7806336\nCholesterol       6.94634560  1.9284815  9.1095206           10.1463380\nTriglycerides     5.92203587  3.6609051  5.6145585            7.7311601\nHDL_Cholesterol  -1.32925931  1.4204124  1.3977476            0.7926818\nLDL_Cholesterol   0.23744178  2.3292662  3.9620622            4.1589452\nVLDL_Cholesterol  6.35132050  4.7634034  6.2262778            8.3446318\nBMI              21.81047332  8.4210010  9.3543430           17.7783627\n                 MeanDecreaseGini\nID                       7.906663\nPatient_Number          11.867330\nGender                   1.709621\nAGE                     25.278841\nUrea                     5.279241\nCreatinine               5.169382\nHbA1c_Level             68.694199\nCholesterol             13.134244\nTriglycerides            9.375600\nHDL_Cholesterol          4.554059\nLDL_Cholesterol          6.381994\nVLDL_Cholesterol         8.575811\nBMI                     54.714782\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot variable importance\nvarImpPlot(rf_model, main = \"Variable Importance\")\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n-----------------------------------------------------------------------------------------------------------------\n\n### K-Nearest Neighbour\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata$CLASS <- as.factor(data$CLASS)  # Ensure CLASS is a factor\n\n# Scale numeric features\nnumeric_columns <- c(\"AGE\", \"Urea\", \"Creatinine\", \"HbA1c_Level\", \"Cholesterol\", \"BMI\")\ntrain_scaled <- scale(train_data[, numeric_columns])\ntest_scaled <- scale(test_data[, numeric_columns])\n\n# Cross-validation to find the optimal k\nset.seed(42)\nerror<-rep(NA,20) # Placeholder\n\nfor (i in 1:20) {\n  # Perform KNN\n  knn_pred <- knn(train = train_scaled, test = test_scaled, cl = train_data$CLASS, k = i)\n  \n  # Calculate test error\n  error[i] <- mean(knn_pred != test_data$CLASS)\n}\n\nerror_df <- data.frame(\n  K = 1:20,                   # Number of neighbors\n  Error = error               # Test error rates\n)\n\n# Add Accuracy (1 - Error) to the data frame\nerror_df$Accuracy <- 1 - error_df$Error\n\n# Plot accuracy vs. K using ggplot2\nggplot(error_df, aes(x = K, y = Accuracy)) +\n  geom_line(color = \"blue\") +\n  geom_point() +\n  ggtitle(\"Accuracy vs K for KNN\") +\n  xlab(\"Number of Neighbors (K)\") +\n  ylab(\"Accuracy\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(error_df, aes(x = K, y = Error)) +\n  geom_line(color = \"blue\") +\n  geom_point() +\n  ggtitle(\"Error vs K for KNN\") +\n  xlab(\"Number of Neighbors (K)\") +\n  ylab(\"Error\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Find the minimum error and corresponding K value\nmin_error <- min(error_df$Error)\noptimal_k <- error_df$K[which.min(error_df$Error)]\n\n# Print the results\nprint(paste(\"Minimum Error:\", round(min_error, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Minimum Error: 0.0404\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Optimal K:\", optimal_k))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Optimal K: 1\"\n```\n\n\n:::\n:::\n\n\n-----------------------------------------------------------------------------------------------------------------\n\n### Decision Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Decision Tree\n\n# Fit a Decision Tree model\ndt_model <- rpart(CLASS ~ ., data = train_data, method = \"class\")\n\n# Plot the Decision Tree\nfancyRpartPlot(dt_model)\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Make predictions on the test data\npredictions <- predict(dt_model, test_data, type = \"class\")\n\n# Evaluate the model\nconf_matrix <- confusionMatrix(predictions, test_data$CLASS)\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   N   P   Y\n         N  19   0   2\n         P   0  10   0\n         Y   1   0 166\n\nOverall Statistics\n                                          \n               Accuracy : 0.9848          \n                 95% CI : (0.9564, 0.9969)\n    No Information Rate : 0.8485          \n    P-Value [Acc > NIR] : 5.887e-11       \n                                          \n                  Kappa : 0.9441          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: N Class: P Class: Y\nSensitivity           0.95000  1.00000   0.9881\nSpecificity           0.98876  1.00000   0.9667\nPos Pred Value        0.90476  1.00000   0.9940\nNeg Pred Value        0.99435  1.00000   0.9355\nPrevalence            0.10101  0.05051   0.8485\nDetection Rate        0.09596  0.05051   0.8384\nDetection Prevalence  0.10606  0.05051   0.8434\nBalanced Accuracy     0.96938  1.00000   0.9774\n```\n\n\n:::\n\n```{.r .cell-code}\n# Optional: Print overall accuracy\naccuracy <- conf_matrix$overall[\"Accuracy\"]\ncat(\"Accuracy:\", accuracy, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAccuracy: 0.9848485 \n```\n\n\n:::\n:::\n\n\n\n-----------------------------------------------------------------------------------------------------------------\n\n#### Use of BMI thresholds to classify individuals into non-diabetic, pre-diabetic, and diabetic categories, and the role BMI plays in predicting diabetes progression across these classes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Density plot to compare BMI distribution\nggplot(data, aes(x = BMI, fill = CLASS)) +\n  geom_density(alpha = 0.5) +\n  scale_x_continuous(breaks=seq(0,48,by=2))+\n  labs(title = \"BMI Density Plot by CLASS\", x = \"BMI\", y = \"Density\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n-----------------------------------------------------------------------------------------------------------------\n\n#### Gender-specific differences in clinical markers or diabetes class distributions?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation Heatmaps by Gender with Values in Boxes\n# Subset data by gender\ndata_male <- subset(data, Gender == \"M\")\ndata_female <- subset(data, Gender == \"F\")\n  \n# Calculate correlation matrix\ncorrelation_matrix_male <- cor(data_male[, c(\"HbA1c_Level\", \"BMI\", \"HDL_Cholesterol\", \"LDL_Cholesterol\", \"Triglycerides\", \"AGE\", \"Urea\", \"Creatinine\")], use = \"complete.obs\")\n\ncorrelation_matrix_female <- cor(data_female[, c(\"HbA1c_Level\", \"BMI\", \"HDL_Cholesterol\", \"LDL_Cholesterol\", \"Triglycerides\", \"AGE\", \"Urea\", \"Creatinine\")], use = \"complete.obs\")\n  \n# Plot heatmap with values\ncorrplot(correlation_matrix_male, method = \"color\", \n           col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200), \n           addCoef.col = \"black\", # Add values to the boxes in black\n           tl.col = \"black\",      # Labels in black\n           tl.cex = 0.8,          # Adjust label size\n           number.cex = 0.7,      # Adjust coefficient size\n           title = paste(\"Correlation Heatmap for Gender: M\"), mar = c(0, 0, 1, 0))\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncorrplot(correlation_matrix_female, method = \"color\", \n           col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200), \n           addCoef.col = \"black\", # Add values to the boxes in black\n           tl.col = \"black\",      # Labels in black\n           tl.cex = 0.8,          # Adjust label size\n           number.cex = 0.7,      # Adjust coefficient size\n           title = paste(\"Correlation Heatmap for Gender: F\"), mar = c(0, 0, 1, 0))\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#linear model\nlm_female <- lm(Creatinine~Urea, data_female)\nlm_male <- lm(Creatinine~Urea, data_male)\n\n\n# Scatterplot for males\nplot_male <- ggplot(data_male, aes(x = Urea, y = Creatinine)) +\n  geom_point(color = \"blue\", alpha = 0.7) +\n  geom_abline(intercept = coef(lm_male)[1], slope = coef(lm_male)[2], color = \"black\") +\n  labs(title = \"Scatterplot of Urea vs Creatine (Males)\",\n       x = \"Urea\", y = \"Creatine\") +\n  theme_minimal()\n\n# Scatterplot for females\nplot_female <- ggplot(data_female, aes(x = Urea, y = Creatinine)) +\n  geom_point(color = \"red\", alpha = 0.7) +\n  geom_abline(intercept = coef(lm_female)[1], slope = coef(lm_female)[2], color = \"black\") +\n  labs(title = \"Scatterplot of Urea vs Creatine (Females)\",\n       x = \"Urea\", y = \"Creatine\") +\n  theme_minimal()\n\n# Arrange the plots side by side\ngrid.arrange(plot_male, plot_female, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n```{.r .cell-code}\n##########\n\n#optimize\n\n# Fit a polynomial regression model (degree 2)\npoly_model <- lm(Creatinine ~ poly(Urea, 2), data = data_female)\n\n# Make predictions\ndata_female$Cr_pred <- predict(poly_model, newdata = data_female)\n\n# Evaluate the model: Mean Squared Error and R-squared\nmse_poly <- mean((data_female$Creatinine - data_female$Cr_pred)^2)\nr2_poly <- summary(poly_model)$r.squared\n\n# Visualization\nggplot(data_female, aes(x = Urea, y = Creatinine)) +\n  geom_point(alpha = 0.7, color = \"purple\", label = \"Data Points\") +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x, 2), color = \"blue\", se = FALSE, label = \"Polynomial Regression Curve\") +\n  labs(title = \"Optimized Polynomial Regression: Urea vs. Creatinine (Gender: F)\",\n       x = \"Urea\", y = \"Creatinine\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  geom_line(aes(y = Cr_pred), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](FinalCode_files/figure-html/unnamed-chunk-14-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# Print evaluation metrics\ncat(\"Evaluation metrics\\n\",\n    \"Mean Squared Error (MSE):\", mse_poly, \"\\n\",\n    \"R-squared (R²):\",r2_poly,\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEvaluation metrics\n Mean Squared Error (MSE): 272.8252 \n R-squared (R²): 0.8057451 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fitting the polynomial model\npoly_model <- lm(Creatinine ~ poly(Urea, 2), data = data_female)\n\n# Predictions\npredictions <- predict(poly_model, newdata = data_female)\n\n# Residuals\nresiduals <- data_female$Creatinine - predictions\n\n# Metrics\nmae <- mean(abs(residuals))\nmape <- mean(abs(residuals / data_female$Creatinine)) * 100\naccuracy <- 100 - mape\nr_squared <- summary(poly_model)$r.squared\n\n# Output the results\ncat(\"Results for Females\\n\",\n    \"---------------------\\n\",\n    \"Mean Absolute Error (MAE):\", mae, \"\\n\",\n    \"Mean Absolute Percentage Error (MAPE):\", mape, \"%\\n\",\n    \"Model Accuracy:\", accuracy, \"%\\n\",\n    \"R-squared (R²):\", r_squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResults for Females\n ---------------------\n Mean Absolute Error (MAE): 11.85335 \n Mean Absolute Percentage Error (MAPE): 24.38884 %\n Model Accuracy: 75.61116 %\n R-squared (R²): 0.8057451 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fitting the polynomial model for males\npoly_model_male <- lm(Creatinine ~ poly(Urea, 2), data = data_male)\n\n# Predictions for males\npredictions_male <- predict(poly_model_male, newdata = data_male)\n\n# Residuals for males\nresiduals_male <- data_male$Creatinine - predictions_male\n\n# Metrics for males\nmae_male <- mean(abs(residuals_male))  # Mean Absolute Error\nmape_male <- mean(abs(residuals_male / data_male$Creatinine)) * 100  # Mean Absolute Percentage Error\naccuracy_male <- 100 - mape_male  # Accuracy\nr_squared_male <- summary(poly_model_male)$r.squared  # R-squared\n\n# Output the results for males\ncat(\"Results for Males\\n\",\n    \"---------------------\\n\",\n    \"Mean Absolute Error (MAE) for Males:\", mae_male, \"\\n\",\n    \"Mean Absolute Percentage Error (MAPE) for Males:\", mape_male, \"%\\n\",\n    \"Model Accuracy for Males:\", accuracy_male, \"%\\n\",\n    \"R-squared (R²) for Males:\", r_squared_male, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResults for Males\n ---------------------\n Mean Absolute Error (MAE) for Males: 26.18579 \n Mean Absolute Percentage Error (MAPE) for Males: 32.58835 %\n Model Accuracy for Males: 67.41165 %\n R-squared (R²) for Males: 0.3310646 \n```\n\n\n:::\n:::\n",
    "supporting": [
      "FinalCode_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}